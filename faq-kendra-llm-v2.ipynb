{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c1f3f6-a06d-40e8-990f-2fb6515f764e",
   "metadata": {},
   "source": [
    "# LLMs in a Box for Search\n",
    "1. Create a SageMaker Studio Domain if you don't have one\n",
    "2. Open SageMaker Studio under the user you plan to launch this applicatio\n",
    "3. Either upload this notebook, or clone the repository: [repo](https://github.com/chaeAclark/literate-eureka.git)\n",
    "4. Open the notebook `LLM and Kendra in a box.ipynb`\n",
    "5. You can run the entire notebook by clicking Run > Run All Cells\n",
    "6. Alternatively, you can run the cells individually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a7499-d5bb-4fc8-b05e-b0ebf9feaa80",
   "metadata": {},
   "source": [
    "### Terminal Installation\n",
    "You need to ensure you have installed all needed packages in the terminal you are using.\n",
    "1. boto3\n",
    "2. streamlit\n",
    "3. pdf2image\n",
    "4. ai21[SM]\n",
    "5. Pillow\n",
    "6. pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577093f2-cd73-453c-9497-671a0e29acc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "boto3\n",
    "streamlit\n",
    "pdf2image\n",
    "ai21[SM]\n",
    "Pillow\n",
    "pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ccfabe-ed9e-40b3-9b85-d5e260a103ef",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964a254-8b04-49c0-8224-d1e8e24fd33f",
   "metadata": {},
   "source": [
    "#### Update SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e1d828-cf2e-4eac-af5c-81fc3a552ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.142 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3680ea2-1228-41a9-8b0f-4dd128657e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./botocore-1.29.142-py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore==1.29.142) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore==1.29.142) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore==1.29.142) (1.26.16)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.29.142) (1.14.0)\n",
      "botocore is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Processing ./boto3-1.26.142-py3-none-any.whl\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.142 in /opt/conda/lib/python3.7/site-packages (from boto3==1.26.142) (1.29.142)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3==1.26.142) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3==1.26.142) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.142->boto3==1.26.142) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.142->boto3==1.26.142) (1.26.16)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.142->boto3==1.26.142) (1.14.0)\n",
      "boto3 is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Processing ./awscli-1.27.142-py3-none-any.whl\n",
      "Requirement already satisfied: botocore==1.29.142 in /opt/conda/lib/python3.7/site-packages (from awscli==1.27.142) (1.29.142)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /opt/conda/lib/python3.7/site-packages (from awscli==1.27.142) (0.16)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from awscli==1.27.142) (0.6.1)\n",
      "Collecting PyYAML<5.5,>=3.10 (from awscli==1.27.142)\n",
      "  Using cached PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from awscli==1.27.142) (0.4.3)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /opt/conda/lib/python3.7/site-packages (from awscli==1.27.142) (4.7.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore==1.29.142->awscli==1.27.142) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore==1.29.142->awscli==1.27.142) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore==1.29.142->awscli==1.27.142) (1.26.16)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.8,>=3.1.2->awscli==1.27.142) (0.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.29.142->awscli==1.27.142) (1.14.0)\n",
      "awscli is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "Installing collected packages: PyYAML\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.173.0 requires PyYAML~=6.0, but you have pyyaml 5.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-5.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install botocore-1.29.142-py3-none-any.whl\n",
    "!pip install boto3-1.26.142-py3-none-any.whl\n",
    "!pip install awscli-1.27.142-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019b8e28-45ad-4988-84f4-499d2ad6e38b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "738750c9-b02a-41be-aa41-05ddd737cb34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '6b869bfb-a2b5-4f03-aebc-8247fdf24ff3',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Sun, 23 Jul 2023 20:22:20 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '861',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '6b869bfb-a2b5-4f03-aebc-8247fdf24ff3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'modelSummaries': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-tg1-large',\n",
       "   'modelId': 'amazon.titan-tg1-large'},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-e1t-medium',\n",
       "   'modelId': 'amazon.titan-e1t-medium'},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl',\n",
       "   'modelId': 'stability.stable-diffusion-xl'},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-grande-instruct',\n",
       "   'modelId': 'ai21.j2-grande-instruct'},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-jumbo-instruct',\n",
       "   'modelId': 'ai21.j2-jumbo-instruct'},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1',\n",
       "   'modelId': 'anthropic.claude-instant-v1'},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v1',\n",
       "   'modelId': 'anthropic.claude-v1'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure your AWS credentials using the aws configure command, or pass them to the\n",
    "# boto3 client\n",
    "import boto3\n",
    "bedrock = boto3.client('bedrock' , 'us-east-1', endpoint_url='https://bedrock.us-east-1.amazonaws.com')\n",
    "bedrock.list_foundation_models()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a065079d-8be7-479a-ad47-335c14a2a801",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ai21 in /opt/conda/lib/python3.7/site-packages (1.1.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from ai21) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->ai21) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->ai21) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->ai21) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->ai21) (2023.5.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ai21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a85aa9-80f3-444e-9698-de9733c6d18d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8b6ad-9840-483b-8e30-0d4d0d0912df",
   "metadata": {},
   "source": [
    "### General Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3550339b-a1a5-41b5-a64c-2d7eccb9acfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import ai21\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05bc81c-1aef-418f-8e12-c5480c159af9",
   "metadata": {},
   "source": [
    "### SageMaker Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2917d6eb-ee5a-4e1f-95c8-8873372fe090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker as sm\n",
    "\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import model_uris\n",
    "from sagemaker import script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0fdd6-3030-4d0f-97b6-d685c14a8fa5",
   "metadata": {},
   "source": [
    "### Deploy and Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee8680a-3684-4cc7-8dd6-fcadb28542d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sagemaker_session(local_download_dir) -> sm.Session:\n",
    "    \"\"\"\n",
    "    # Create a SageMaker Session\n",
    "    # This function is used to create a SageMaker Session object.\n",
    "    # The SageMaker Session object is used to create a SageMaker Endpoint,\n",
    "    # SageMaker Model, and SageMaker Endpoint Config.\n",
    "    \"\"\"\n",
    "    sagemaker_client = boto3.client(service_name=\"sagemaker\", region_name=boto3.Session().region_name)\n",
    "    session_settings = sm.session_settings.SessionSettings(local_download_dir=local_download_dir)\n",
    "    session = sm.session.Session(sagemaker_client=sagemaker_client, settings=session_settings)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02eb635b-97a2-44b5-b60d-0649e494b5bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = './download_dir'\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6051dd9f-a009-4ca9-ad55-d7db46390136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_df = pd.DataFrame([], columns=['timestamp', 'question', 'response'])\n",
    "chat_df.to_csv('chat.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c60275-6f96-4baf-8058-35eb2bf2d590",
   "metadata": {},
   "source": [
    "### SageMaker Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1519db94-193e-47e0-bc9f-2bd0a8700954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role               = sm.get_execution_role()\n",
    "sagemaker_session  = get_sagemaker_session(model_path) # sm.session.Session()\n",
    "region             = sagemaker_session._region_name\n",
    "\n",
    "# These are needed to show where the streamlit app is hosted\n",
    "sagemaker_metadata = json.load(open('/opt/ml/metadata/resource-metadata.json', 'r'))\n",
    "domain_id          = sagemaker_metadata['DomainId']\n",
    "resource_name      = sagemaker_metadata['ResourceName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6786fe8-304f-496a-a094-4a4a15d483e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock = boto3.client(service_name='bedrock',region_name='us-east-1',endpoint_url='https://bedrock.us-east-1.amazonaws.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473a009-0de5-4336-b600-043391652d61",
   "metadata": {},
   "source": [
    "### Boto Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78ce7521-cb17-4e82-886d-f2522d1f1e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# index_name = 's3-sagemaker-faq'#'kgnraham-faq'\n",
    "# index_id   = '53b9aca5-b61c-4b59-92fb-ff8c68e4ca39'#'3b1f740a-7360-4746-9565-998e0580ea57'\n",
    "index_name = 'kgnraham-faq'\n",
    "index_id   = '3b1f740a-7360-4746-9565-998e0580ea57'\n",
    "\n",
    "bucket     = 'sagemaker-us-east-1-715253196401'\n",
    "prefix     = 'textract'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190d2f3-9e29-49f8-9366-6734d6d98909",
   "metadata": {},
   "source": [
    "# Model\n",
    "The following section will deploy the JumpStart model `flan-###`. There are additional steps required if launching 3rd-party proprietary models. These steps are detailed in another section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507d925-e504-4947-9745-2849324bed6d",
   "metadata": {},
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95b8511f-19a0-4a04-9469-21b9eb9906f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available text2text Models:\n",
      "--------------------------------\n",
      "huggingface-text2text-bart4csc-base-chinese\n",
      "huggingface-text2text-bigscience-t0pp\n",
      "huggingface-text2text-bigscience-t0pp-bnb-int8\n",
      "huggingface-text2text-bigscience-t0pp-fp16\n",
      "huggingface-text2text-flan-t5-base\n",
      "huggingface-text2text-flan-t5-base-samsum\n",
      "huggingface-text2text-flan-t5-large\n",
      "huggingface-text2text-flan-t5-small\n",
      "huggingface-text2text-flan-t5-xl\n",
      "huggingface-text2text-flan-t5-xxl\n",
      "huggingface-text2text-flan-t5-xxl-bnb-int8\n",
      "huggingface-text2text-flan-t5-xxl-fp16\n",
      "huggingface-text2text-flan-ul2-bf16\n",
      "huggingface-text2text-pegasus-paraphrase\n",
      "huggingface-text2text-qcpg-sentences\n",
      "huggingface-text2text-t5-one-line-summary\n"
     ]
    }
   ],
   "source": [
    "filter_value = \"task == text2text\"\n",
    "text_generation_models = list_jumpstart_models(filter=filter_value)\n",
    "print('Available text2text Models:\\n--------------------------------')\n",
    "_ = [print(m) for m in text_generation_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27b902cc-7aa2-416e-a39d-4efa949f236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model that will be deployed is: huggingface-text2text-flan-t5-small\n"
     ]
    }
   ],
   "source": [
    "model_id = text_generation_models[7]\n",
    "model_version = '*'\n",
    "print(f'The model that will be deployed is: {model_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31359a-2373-4c7b-ba5f-9f7e4091d9b9",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd51fb1-b23e-4064-a045-54db4817b7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = name_from_base(f\"LLM-in-a-box-{model_id}\")\n",
    "print(f'Endpoint: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483be2d7-bb22-4d26-a592-5fdae7f208a5",
   "metadata": {},
   "source": [
    "#### Collect Model Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7654cf-9de8-4d69-a890-811d0c5687da",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "\n",
    "model_data = model_uris.retrieve(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "print(f'The image URI is:  {image_uri}')\n",
    "print(f'The model data is: {model_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60571448-807e-4e3f-b652-8526a5779541",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57995fb1-9488-4655-a152-b3ad01a24d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    env={\"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649c67c-5db4-47c5-8067-5d741c268311",
   "metadata": {},
   "source": [
    "#### Deploy Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25faaa-1c69-4e28-9e06-b0829a6f967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_predictor = model.deploy(\n",
    "#     initial_instance_count=1,\n",
    "#     instance_type=instance_type,\n",
    "#     predictor_cls=Predictor,\n",
    "#     endpoint_name=endpoint_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eee911-c201-4d99-8c04-96fcc6fc2af3",
   "metadata": {},
   "source": [
    "#### Test that the model is deployed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90ea6d-86b0-4c22-915b-d5fe17e41e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker-runtime', region_name=region)\n",
    "input_question = 'Tell me the steps to make a pizza:'\n",
    "payload = {\n",
    "    \"text_inputs\": input_question,\n",
    "    \"max_length\": 50,\n",
    "    \"max_time\": 50,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "response = sagemaker.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload).encode('utf-8')\n",
    ")\n",
    "output_answer = json.loads(response['Body'].read().decode('utf-8'))[\"generated_texts\"][0]\n",
    "print(output_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64695926-b8f5-4e6a-a6fb-1bb331577759",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How to deploy 3rd-party Foundation Models\n",
    "1. Gain access to the foundation models\n",
    "    1. Go to the SageMaker Console\n",
    "    2. There will be a tab for JumpStart > Foundation Models\n",
    "    3. You must request access if you do not already have it\n",
    "2. Select the Foundation you would like to deploy\n",
    "3. Click `Subscribe` in the top-right corner\n",
    "4. After completing, this will allow you to open a notebook that lets you deploy the model\n",
    "5. Open the notebook\n",
    "6. You run this notebook to deploy the model, the caveat is that you must have access to any instance you choose to run.\n",
    "    1. For AI21 Summarization model, you can use something like: ml.g4dn.12xlarge\n",
    "    2. For AI21 Grande Instruct, you can use: ml.g5.24xlarge\n",
    "    3. For AI21 Jumbo Instruct, you can use: ml.g5.48xlarge\n",
    "    4. These were tested to work as of 2023-05-16\n",
    "    5. Collect these endpoint names and use them in the application_metadata JSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b66f1-2750-4569-8750-618b002a4735",
   "metadata": {},
   "source": [
    "# Streamlit UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c4fbf7-82e9-4770-839f-ebd98c23a2b5",
   "metadata": {},
   "source": [
    "### Record any parameters that need to be passed to the Streamlit app\n",
    "App Metadata Structure:\n",
    "#### application_metadata\n",
    " - models: a dictionary that contains the model display name, SageMaker endpoint name, and the model type (Currently 'sm' or 'ai21')\n",
    "   - name\n",
    "   - endpoint\n",
    "   - type\n",
    " - summary_model: the summary model endpoint name\n",
    " - region: the region (us-east-1 etc)\n",
    " - role: the permissions for the application. it should include (SageMaker, Textract, and Kendra access)\n",
    " - datastore: a dictionary that contains the bucket and folder prefix used to store document data\n",
    "   - bucket\n",
    "   - prefix\n",
    " - kendra: a dictionary that contains information on the Kendra index to be used when searching\n",
    "   - index_id\n",
    "   - index_name\n",
    "   - index_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de01c0f-368e-41e0-9779-509250c256eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81e1c2bf-58b6-4339-b2d7-ef7e33655f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "application_metadata = {\n",
    "    'models':[\n",
    "        #{'name':'FLAN-XL', 'endpoint':'jumpstart-example-huggingface-text2text-2023-06-10-02-04-21-787', 'type':'sm'},\n",
    "        # {'name':'Jurassic-2-Jumbo', 'endpoint':'j2-jumbo-instruct-arcesium', 'type':'ai21'},\n",
    "        {'name':'ANTROPIC-CLAUDE-v1', 'endpoint':'anthropic.claude-v1', 'type':'anthropic'},\n",
    "        {'name':'AMAZON-TITAN-L', 'endpoint':'amazon.titan-tg1-large', 'type':'amazon'},\n",
    "        {'name':'AI21-JUMBO-GRANDE-INSTRUCT', 'endpoint':'ai21.j2-grande-instruct', 'type':'ai21'}\n",
    "    ],\n",
    "    #'summary_model':'summarize',\n",
    "    'region':region,\n",
    "    'role':role,\n",
    "    'datastore':\n",
    "        {'bucket':bucket, 'prefix':prefix},\n",
    "    'kendra':\n",
    "        {'index_id':index_id, 'index_name':index_name, 'index_description':''},\n",
    "}\n",
    "json.dump(application_metadata, open('application_metadata.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec73e1-bc9a-4647-9d57-bd4170f3b949",
   "metadata": {},
   "source": [
    "### Write the Streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1da3bc03-01df-4a28-bb63-77a47aebe367",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app_faq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app_faq.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import ai21\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "#from pdf2image import convert_from_bytes\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "bedrock = boto3.client('bedrock', region_name='us-east-1', endpoint_url='https://bedrock.us-east-1.amazonaws.com')\n",
    "\n",
    "APP_MD    = json.load(open('application_metadata.json', 'r'))\n",
    "MODELS    = {d['name']: d['endpoint'] for d in APP_MD['models']}\n",
    "#MODEL_SUM = APP_MD['summary_model']\n",
    "REGION    = APP_MD['region']\n",
    "BUCKET    = APP_MD['datastore']['bucket']\n",
    "PREFIX    = APP_MD['datastore']['prefix']\n",
    "KENDRA_ID = APP_MD['kendra']['index_id']\n",
    "#CONTEXT = deque([''], maxlen=10)\n",
    "\n",
    "S3            = boto3.client('s3', region_name=REGION)\n",
    "TEXTRACT      = boto3.client('textract', region_name=REGION)\n",
    "KENDRA        = boto3.client('kendra', region_name=REGION)\n",
    "SAGEMAKER     = boto3.client('sagemaker-runtime', region_name=REGION)\n",
    "\n",
    "CHAT_FILENAME = 'chat.csv'\n",
    "params = {'file':'','action_name':'','endpoint':'', 'max_len':0, 'top_p':0, 'temp':0, 'model_name':''}\n",
    "\n",
    "\n",
    "def query_endpoint(endpoint_name, prompt_data,params):\n",
    "    accept='application/json'\n",
    "    contentType='application/json'\n",
    "    \n",
    "    # if 'huggingface' in endpoint_name:\n",
    "    #     response = SAGEMAKER.invoke_endpoint(\n",
    "    #         EndpointName=endpoint_name,\n",
    "    #         ContentType=contentType,\n",
    "    #         Body=json.dumps(payload).encode('utf-8')\n",
    "    #     )\n",
    "    #     output_answer = json.loads(response['Body'].read().decode('utf-8'))[\"generated_texts\"][0]\n",
    "    if 'claude' in endpoint_name:\n",
    "        body=json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\":params['max_len'],\"temperature\":params['temp'],\"top_p\":params['top_p']})\n",
    "        response = bedrock.invoke_model(\n",
    "            body=body, \n",
    "            modelId='anthropic.claude-instant-v1', \n",
    "            accept=accept, \n",
    "            contentType=contentType\n",
    "        )\n",
    "        #print(response)\n",
    "        output_answer = json.loads(response.get('body').read()).get('completion')\n",
    "    elif 'titan' in endpoint_name:\n",
    "        #print(json.dumps(prompt_data))\n",
    "        body=json.dumps({\"inputText\": prompt_data,\"textGenerationConfig\": {\n",
    "                          \"maxTokenCount\": params['max_len'],\n",
    "                          \"temperature\":params['temp'],\n",
    "                          \"topP\":params['top_p']\n",
    "                         }})\n",
    "        response = bedrock.invoke_model(\n",
    "            body=body, \n",
    "            modelId='amazon.titan-tg1-large', \n",
    "            accept=accept, \n",
    "            contentType=contentType\n",
    "        )\n",
    "        output_answer = json.loads(response.get('body').read()).get('results')[0].get('outputText')\n",
    "    elif 'j2' in endpoint_name:\n",
    "        body = json.dumps({\"prompt\": prompt_data,\"maxTokens\":params['max_len'],\"temperature\":params['temp'],\"topP\":params['top_p']})\n",
    "        print(json.dumps(prompt_data))\n",
    "        response = bedrock.invoke_model(\n",
    "            body=body, \n",
    "            modelId= 'ai21.j2-grande-instruct', # change this to use a different version from the model provider, \n",
    "            accept=accept, \n",
    "            contentType=contentType)\n",
    "        output_answer = json.loads(response.get('body').read()).get('completions')[0].get('data').get('text')\n",
    "    \n",
    "    #print(output_answer)\n",
    "    return str(output_answer)\n",
    "\n",
    "def query_index(query):\n",
    "    response = KENDRA.query(\n",
    "        QueryText = query,\n",
    "        IndexId = KENDRA_ID\n",
    "        \n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def extract_text(bucket, filepath):\n",
    "    response = TEXTRACT.start_document_text_detection(DocumentLocation={'S3Object': {'Bucket':bucket, 'Name':filepath}})\n",
    "    text = TEXTRACT.get_document_text_detection(JobId=response['JobId'])\n",
    "    i = 0\n",
    "    while text['JobStatus'] != 'SUCCEEDED':\n",
    "        time.sleep(5)\n",
    "        i += 1\n",
    "        text = TEXTRACT.get_document_text_detection(JobId=response['JobId'])\n",
    "        if i >= 10:\n",
    "            text = ''\n",
    "            break\n",
    "    text = '\\n'.join([t['Text'] for t in text['Blocks'] if t['BlockType']=='LINE'])\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_document(file_bytes):\n",
    "    # try:\n",
    "    #     images = convert_from_bytes(file_bytes)\n",
    "    #     image_page_1 = images[0].convert('RGB')\n",
    "    #     st.image(image_page_1)\n",
    "    # except:\n",
    "    #     st.write('Cannot display image. Ensure that you have poppler-utils installed.')\n",
    "    \n",
    "    with open('doc.pdf', 'wb') as fp:\n",
    "        fp.write(file_bytes)\n",
    "    with open('doc.pdf', 'rb') as fp:\n",
    "        S3.upload_fileobj(fp, BUCKET, PREFIX+'/doc.pdf')\n",
    "    time.sleep(2)\n",
    "    text = extract_text(BUCKET, PREFIX+'/doc.pdf')\n",
    "    return text\n",
    "\n",
    "\n",
    "def summarize_context(context, params):\n",
    "    try:\n",
    "        prompt_data =\"\"\"'\"\"\"+context+\"\"\"\\n\"\"\"+\"summarize the context\"+\"\"\"'\"\"\"\n",
    "        output_summary = query_endpoint(params['endpoint'],prompt_data, params)\n",
    "        return output_summary\n",
    "            \n",
    "    except:\n",
    "        return 'No summarization endpoint connected'\n",
    "\n",
    "def action_qna(params):\n",
    "    st.title('Ask Questions of your Model')\n",
    "    try:\n",
    "        chat_df = pd.read_csv(CHAT_FILENAME)\n",
    "        \n",
    "    except:\n",
    "        chat_df = pd.DataFrame([], columns=['timestamp', 'question', 'response'])\n",
    "    kendra_links = []\n",
    "    \n",
    "    input_question = st.text_input('**Please ask a question:**', '')\n",
    "    if st.button('Send Question') and len(input_question) > 3:\n",
    "        response = query_index(input_question)\n",
    "        #print(\"response:\",response['ResultItems'])\n",
    "        for sr in response['ResultItems']:\n",
    "            # kendra_links.append(sr['DocumentURI'])\n",
    "            if sr['ScoreAttributes']['ScoreConfidence'] == 'HIGH':\n",
    "                kendra_links.append(sr['DocumentURI'])\n",
    "                # st.write(f\"[Link to Source Document]({sr['DocumentURI']})\")\n",
    "                # st.write(f\"**[{sr['ScoreAttributes']['ScoreConfidence']}]** | {sr['DocumentTitle']['Text']} [Link to Source Document]({sr['DocumentURI']})\")\n",
    "                # st.write(sr['DocumentExcerpt']['Text'])\n",
    "                # # st.write('---')\n",
    "        print(\"Kendra Links:\",kendra_links)        \n",
    "        # kendra_links = list(set(kendra_links))\n",
    "        kendra_context = '\\n'.join([sr['DocumentTitle']['Text']  for sr in response['ResultItems'] if sr['ScoreAttributes']['ScoreConfidence'] == 'HIGH'])\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        # context = '\\n'.join(['Context: ' + str(r.question) + '\\nResponse: ' + str(r.response) + '\\n' for idx,r in chat_df.iloc[-1:].iterrows()])\n",
    "        # context = context + kendra_context\n",
    "        print(\"kendra context:\",kendra_context)\n",
    "        #payload = {\n",
    "        #     \"text_inputs\": context + '\\n' + input_question, #input_question,\n",
    "        #     \"max_length\": params['max_len'],\n",
    "        #     \"max_time\": 50,\n",
    "        #     \"num_return_sequences\": 1,\n",
    "        #     \"top_k\": 50,\n",
    "        #     \"top_p\": params['top_p'],\n",
    "        #     \"do_sample\": True,\n",
    "        # }\n",
    "        prompt_data =\"\"\"'\"\"\"+'Read the following passage and answer the questions that follow:\\n'+kendra_context+\"\"\"\\n\"\"\"+'Questions:'+input_question+\"\"\"\\n\"\"\"+'Answer:'+\"\"\"'\"\"\"\n",
    "\n",
    "\n",
    "        output_answer = query_endpoint(params['endpoint'], prompt_data, params)\n",
    "        st.text_area('Response:', output_answer)\n",
    "        for each_link in kendra_links[0:1]:   #        \n",
    "            st.write(f\"[Link to Source Document]({each_link})\") \n",
    "        chat_df.loc[len(chat_df.index)] = [timestamp, input_question, output_answer]\n",
    "        chat_df.tail(5).to_csv(CHAT_FILENAME, index=False)\n",
    "                    \n",
    "    st.subheader('Recent Questions:')\n",
    "    for idx,row in chat_df.iloc[::-1].head(5).iterrows():\n",
    "        st.write(f'**{row.timestamp}**')\n",
    "        st.write(row.question)\n",
    "        st.write(row.response)\n",
    "        st.write('---')\n",
    "\n",
    "\n",
    "# def action_search(params):\n",
    "#     st.title('Ask Questions of your Document')\n",
    "#     #col2 = st.columns(1)\n",
    "#     #with col2:\n",
    "#     input_question = st.text_input('**Please ask a question of a loaded document:**', '')\n",
    "#     if st.button('Send Question') and len(input_question) > 3:\n",
    "#         # LLM \n",
    "#         payload = {\n",
    "#             \"text_inputs\": input_question,\n",
    "#             #\"max_length\": params['max_len'],\n",
    "#             \"max_time\": 50,\n",
    "#             #\"maxTokens\": params['max_len'],\n",
    "#             \"num_return_sequences\": 1,\n",
    "#             \"top_k\": 50,\n",
    "#             \"temperature\":params['temp'],\n",
    "#             \"top_p\": params['top_p'],\n",
    "#             \"do_sample\": True,\n",
    "#         }\n",
    "#         if params[\"model_name\"] == \"Bedrock Titan Model\":\n",
    "#                 output_answer = query_bedrock_endpoint(payload)\n",
    "\n",
    "#         if \"FLAN\" in params[\"model_name\"]:\n",
    "#             #del payload['maxTokens']\n",
    "#             payload['max_length'] = params['max_len']\n",
    "#             output_answer = query_endpoint(params['endpoint'], payload)\n",
    "\n",
    "\n",
    "#         elif \"Jumbo\" in params[\"model_name\"]:\n",
    "#             #del payload['max_length']\n",
    "#             payload['maxTokens'] = params['max_len']\n",
    "#             output_answer = query_endpoint(params['endpoint'], payload)\n",
    "#         st.text_area('Response:', output_answer,height = 400)\n",
    "\n",
    "\n",
    "def action_doc(params):\n",
    "    st.title('Ask Questions of your Document')\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        file = st.file_uploader('Upload a PDF file', type=['pdf'])\n",
    "        if file is not None:\n",
    "            context = load_document(file.read())\n",
    "            if st.button('Summarize'):\n",
    "                st.write('**Summary:**')\n",
    "                st.write(summarize_context(context, params))\n",
    "    with col2:\n",
    "        input_question = st.text_input('**Please ask a question of a loaded document:**', '')\n",
    "        if st.button('Send Question') and len(input_question) > 3:\n",
    "            prompt_data =\"\"\"'\"\"\"+'Read the following passage and answer the questions that follow:\\n'+context+\"\"\"\\n\"\"\"+'Questions:'+input_question+\"\"\"\\n\"\"\"+'Answer:'+\"\"\"'\"\"\"\n",
    "            output_answer = query_endpoint(params['endpoint'],prompt_data, params)\n",
    "            st.text_area('Response:', output_answer)\n",
    "\n",
    "\n",
    "def app_sidebar():\n",
    "    with st.sidebar:\n",
    "        st.write('## How to use:')\n",
    "        description = \"\"\"Welcome to our LLM tool extraction and query answering application. With this app, you can aske general question, \n",
    "        ask questions of a specific document, or intelligently search an internal document corpus. By selection the action you would like to perform,\n",
    "         you can ask general questions, or questions of your document. Additionally, you can select the model you use, to perform real-world tests to determine model strengths and weakneses.\"\"\"\n",
    "        st.write(description)\n",
    "        st.write('---')\n",
    "        st.write('### User Preference')\n",
    "        if st.button('Clear Context'):\n",
    "            pd.DataFrame([], columns=['timestamp', 'question', 'response']).to_csv(CHAT_FILENAME, index=False)\n",
    "        action_name = st.selectbox('Choose Activity', options=['Question/Answer', 'Document Query' ]) #'Corpus Search',\n",
    "        # if action_name == 'Corpus Search':\n",
    "        #     while file is not None:\n",
    "        #         file = st.file_uploader('Upload a PDF file', type=['pdf'])\n",
    "        model_name = st.selectbox('Select Model', options=MODELS.keys())\n",
    "        max_len = st.slider('Max Length', min_value=50, max_value=1500, value=150, step=10)\n",
    "        top_p = st.slider('Top p', min_value=0., max_value=1., value=1., step=.01)\n",
    "        temp = st.slider('Temperature', min_value=0.01, max_value=1., value=1., step=.01)\n",
    "        st.write('---')\n",
    "        st.write('## FAQ')\n",
    "        st.write(f'**1. Where is the model stored?** \\n\\nThe current model is: `{model_name}` and is running within your account.')\n",
    "        st.write(f'**2. Where is my data stored?**\\n\\n. Currently the queries you make to the endpoint are not stored, but you can enaable this by capturing data from your endpoint.')\n",
    "        st.write('---')\n",
    "        params['action_name']=action_name\n",
    "        params['endpoint']=MODELS[model_name]\n",
    "        params['max_len']=max_len\n",
    "        params['top_p']=top_p\n",
    "        params['temp']=temp\n",
    "        params['model_name']=model_name\n",
    "       \n",
    "        # params = {'file':'','action_name':action_name,'endpoint':MODELS[model_name], 'max_len':max_len, 'top_p':top_p, 'temp':temp, 'model_name':model_name}\n",
    "        return params\n",
    "\n",
    "\n",
    "def main():\n",
    "    params = app_sidebar()\n",
    "\n",
    "    endpoint=params['endpoint']\n",
    "    # if params['action_name'] == 'Corpus Search':\n",
    "    #     params = action_search(params)\n",
    "    if params['action_name'] == 'Question/Answer':\n",
    "        params = action_qna(params)\n",
    "    elif params['action_name'] == 'Document Query':\n",
    "        params = action_doc(params)\n",
    "    else:\n",
    "        raise ValueError('Invalid action name.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c91914-9a00-4961-bed8-37a7e543adff",
   "metadata": {},
   "source": [
    "## Start App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10ab32-44a5-47ef-8e6c-c6825dd8e6ee",
   "metadata": {},
   "source": [
    "### Run Streamlit\n",
    "To run the application:\n",
    "1. Select File > New > Terminal\n",
    "2. In the terminal, use the command: `streamlit run app_faq.py --server.runOnSave true`\n",
    "   1. Note: ensure you have installed all required packages\n",
    "3. If this is successful, you will be able to interact with the app by using the web address below\n",
    "4. An important thing to note is that when you run the above command, you should see an output similar to below.\n",
    "5. The port thats  displayed is the same port that MUST be used after the `proxy` folder below.\n",
    "`\n",
    "You can now view your Streamlit app in your browser.\n",
    "\n",
    "  Network URL: http://###.###.###.###:8501\\\n",
    "  External URL: http://###.###.###.###:8501\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31453036-6eff-42da-9b6c-f53cc8b54381",
   "metadata": {},
   "source": [
    "#### Display Link to Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06fa9910-1dc9-4d76-a854-dc5f7fdd73dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://d-smgyfkpaofzr.studio.us-east-1.sagemaker.aws/jupyter/default/proxy/8501/\n"
     ]
    }
   ],
   "source": [
    "print(f'http://{domain_id}.studio.{region}.sagemaker.aws/jupyter/default/proxy/8501/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b072d7-3df3-4878-996e-5acec9a0be55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AI21 Jurassic-2 Jumbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db12029-5912-4a61-82cc-f9211223aff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ai21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4fce8-7227-47f3-a4c9-131ccff3de02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474210ea-d18f-4d7c-8823-449e2ef515ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15830b5-a074-4236-9586-079636b0626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"This is my first book in english from shakespear, The Murder Myster\"\n",
    "input_question = \"who is the author of the book?\"\n",
    "prompt_data =\"\"\"'\"\"\"+'Read the following passage and answer the questions that follow:\\n'+context+\"\"\"\\n\"\"\"+'Questions:'+input_question+\"\"\"\\n\"\"\"+'Answer:'+\"\"\"'\"\"\"\n",
    "print(prompt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ee72a-ec70-46ec-8814-62c8b3b52e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_data = \"s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/\"\n",
    "!mkdir -p faq_data\n",
    "!aws s3 cp --recursive $original_data faq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549f91c-1821-474e-8e90-4f5305674d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {'inputTextTokenCount': 24, 'results': [{'tokenCount': 17, 'outputText': '\\n\\nTextract is a software product that uses AI to extract data from documents.', 'completionReason': None}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc85ce-350d-4a07-91aa-916a3624bd27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.get('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73cfb7-1982-4b41-8b5b-b4d1836615c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "url_list = ['https://aws.amazon.com/jp/sagemaker/?nc=sn&loc=0','https://aws.amazon.com/jp/sagemaker/',\n",
    "           'https://aws.amazon.com/faqs/','https://aws.amazon.com/jp/kendra/faqs/',\n",
    "           'https://aws.amazon.com/kendra/faqs/?nc1=h_ls']\n",
    "\n",
    "re.findall(r'https://aws.amazon.com/[a-z]{2}/*/faqs/?nc1=h_ls',url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e401a-75b3-410f-8490-321f445ca8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
